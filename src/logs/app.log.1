2024-02-18 12:25:16,125 - ERROR - An error occurred when trying to join both dataframes: An error occurred while calling o58.csv.
: ExitCodeException exitCode=-1073741515: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)
	at org.apache.hadoop.util.Shell.run(Shell.java:900)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:1583)

2024-02-18 12:29:06,200 - INFO - The number of paramters is valid.
2024-02-18 12:29:06,200 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:29:06,200 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:29:14,396 - ERROR - An error occurred when trying to join both dataframes: DataFrameWriter.parquet() got an unexpected keyword argument 'header'
2024-02-18 12:32:38,779 - INFO - The number of paramters is valid.
2024-02-18 12:32:38,779 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:32:38,779 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:32:46,943 - ERROR - An error occurred when trying to join both dataframes: No module named 'distutils'
2024-02-18 12:34:30,706 - INFO - The number of paramters is valid.
2024-02-18 12:34:30,706 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:34:30,706 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:34:45,536 - ERROR - An error occurred when trying to join both dataframes: No module named 'distutils'
2024-02-18 12:36:50,084 - INFO - The number of paramters is valid.
2024-02-18 12:36:50,084 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:36:50,100 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:37:04,800 - ERROR - An error occurred when trying to join both dataframes: No module named 'distutils'
2024-02-18 12:40:46,679 - INFO - The number of paramters is valid.
2024-02-18 12:40:46,679 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:40:46,679 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:41:02,122 - ERROR - An error occurred when trying to join both dataframes: No module named 'distutils'
2024-02-18 12:41:02,136 - ERROR - An error occurred when trying to join both dataframes: cannot access local variable 'pandas_df' where it is not associated with a value
2024-02-18 12:43:25,914 - INFO - The number of paramters is valid.
2024-02-18 12:43:25,914 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:43:25,914 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:43:41,219 - ERROR - An error occurred when trying to join both dataframes: cannot access local variable 'pandas_df' where it is not associated with a value
2024-02-18 12:45:40,778 - INFO - The number of paramters is valid.
2024-02-18 12:45:40,778 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:45:40,778 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:45:51,944 - ERROR - An error occurred when trying to join both dataframes: name 'pandas_df' is not defined
2024-02-18 12:49:31,280 - INFO - The number of paramters is valid.
2024-02-18 12:49:31,280 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 12:49:31,280 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 12:49:41,147 - ERROR - An error occurred when trying to join both dataframes: name 'pandas_df' is not defined
2024-02-18 13:35:38,588 - INFO - Closing down clientserver connection
2024-02-18 13:36:40,826 - INFO - The number of paramters is valid.
2024-02-18 13:36:40,826 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 13:36:40,826 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 13:36:54,234 - INFO - Closing down clientserver connection
2024-02-18 14:22:17,876 - INFO - The number of paramters is valid.
2024-02-18 14:22:17,876 - INFO - The path raw_data\dataset_one.csv exists
2024-02-18 14:22:17,876 - INFO - The path raw_data\dataset_two.csv exists
2024-02-18 14:22:29,263 - INFO - Closing down clientserver connection
2024-02-18 14:31:49,642 - INFO - The number of paramters is valid.
2024-02-18 14:31:49,643 - INFO - The path raw_data\dataset_one.csv exists
